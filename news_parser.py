import random
from bs4 import BeautifulSoup
import requests
import main
from pdf_generator import generatePdfFile

last_news = []

def trim_text_to_words(text, max_words):
    words = text.split()
    if len(words) <= max_words:
        return text

    trimmed_words = words[:max_words]
    trimmed_text = " ".join(trimmed_words)

    last_punctuation = max(
        trimmed_text.rfind("."),
        trimmed_text.rfind("!"),
        trimmed_text.rfind("?"),
    )

    if last_punctuation != -1:
        return trimmed_text[:last_punctuation + 1]
    return trimmed_text

def edit_file(current_news, user_id):
    new_title = current_news[0]['title']
    if '(–≤–∏–¥–µ–æ)' in new_title:
        new_title = new_title.replace('(–≤–∏–¥–µ–æ)', '')

    new_body = current_news[0]['text']
    new_body = trim_text_to_words(new_body, 150)

    new_url = last_news[0]['image_url']
    new_timestamp = current_news[0]['publication_timestamp']
    new_source = last_news[0]['source']

    generatePdfFile(
        title=new_title,
        body=new_body,
        image_url=new_url,
        timestamp=new_timestamp,
        source=new_source,
        user_id=user_id
    )

def parse_news(url, user_id):
    headers = {
        "User-Agent": random.choice(main.user_agents)
    }
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        bs4 = BeautifulSoup(response.text, "lxml")
        last_news.clear()

        title_tag = bs4.find("h1", class_="blog-post-title uppercase")
        body_texts = bs4.find("div", class_="blog-post-item").find_all("p") if bs4.find("div",
                                                                                        class_="blog-post-item") else []
        image_tag = bs4.find("img", class_="img-responsive")
        publication_timestamp_tag = bs4.find("span", class_="font-lato localtime")
        source_tag = bs4.find("span", class_="pull-right text-right margin-top-6 font-lato font-style-italic")

        title = title_tag.text.strip() if title_tag else "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω"
        text = "\n".join([p.text.strip() for p in body_texts]) if body_texts else "–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"
        image_url = 'https://rosguard.gov.ru' + image_tag['src'] if image_tag and image_tag.get(
            'src') else "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
        publication_timestamp = publication_timestamp_tag.text.strip() if publication_timestamp_tag else "–î–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
        source = source_tag.text.strip() if source_tag else "–ò—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ —É–∫–∞–∑–∞–Ω"

        new_news = {
            "title": title,
            "text": text,
            "image_url": image_url,
            "publication_timestamp": publication_timestamp,
            "source": source,
        }

        last_news.append(new_news)

        news_text = (
            f"üì∞ <b>{title}</b>\n\n"
            f"üìù <i>{text}</i>\n\n"
            f"üñºÔ∏è <a href='{image_url}'>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</a>\n\n"
            f"üìÖ <b>–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:</b> {publication_timestamp}\n"
            f"üîó <b>–ò—Å—Ç–æ—á–Ω–∏–∫:</b> {source}"
        )

        main.bot.send_message(user_id, news_text, parse_mode="HTML", disable_web_page_preview=False)

    except requests.RequestException as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {url}: {e}")
        main.bot.send_message(user_id, "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")
        main.bot.send_message(user_id, "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–æ–≤–æ—Å—Ç–∏.")
    edit_file(last_news, user_id)